{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------------------\n",
    "# In this test part\n",
    "# 1. We test performance of 3 model we train in the TRAINING PART \n",
    "# 2. Three Model:\n",
    "#    Faster RCNN with shared CNN: VGG16, Resnet50, and Inception V4\n",
    "# 3. Each time we test, we load Config from pickle with stores Config information,\n",
    "#    and weights of model from h5py file.\n",
    "#    TWO CRITICAL PARTS: A and B. you can find them below\n",
    "# ---------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import h5py\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from optparse import OptionParser\n",
    "import time\n",
    "from keras_frcnn import config\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras_frcnn import roi_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new model path? T or F:T\n",
      "filename:config_vgg_16.pickle\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "parse arguments\n",
    "Used in the test process\n",
    "Idea from py-faster-rcnn (python + caffe, https://github.com/rbgirshick/py-faster-rcnn)\n",
    "\"\"\"\n",
    "sys.setrecursionlimit(40000)\n",
    "\n",
    "parser = OptionParser()\n",
    "parser.add_option('-f','--fff',dest = 'fff',default='/Users/zehaodong/Desktop/faster-rcnn-keras')\n",
    "parser.add_option(\"-p\", \"--path\", dest=\"test_path\", help=\"Path to test data.\",default = '/Users/zehaodong/Desktop/faster-rcnn-keras/VOCdevkit')\n",
    "parser.add_option(\"-n\", \"--num_rois\", type=\"int\", dest=\"num_rois\",help=\"Number of ROIs per iteration. Higher means more memory use.\", default=32)\n",
    "choose = input('new model path? T or F:')\n",
    "# ------------------------------------------------------------------------\n",
    "# A: here decide which model to test and visualization\n",
    "#    choose a pickle name: config_vgg_16.pickle, config_best.pickle\n",
    "#                          config_resnet.pickle, config_inception.pickle\n",
    "#    Note: We trained 4 model, since this work is time consuming,\n",
    "#          only vgg16 is able to trined for about 1000 epoch.\n",
    "#          others are only trained for about 20 epoch each.\n",
    "# ------------------------------------------------------------------------\n",
    "if choose == 'T':\n",
    "    con = input('filename:')\n",
    "else:\n",
    "    con = \"config.pickle\"\n",
    "parser.add_option(\"--config_filename\", dest=\"config_filename\", help=\"Location to read the metadata related to the training (generated when training).\",default= con)\n",
    "parser.add_option(\"--network\", dest=\"network\", help=\"raw feature extraction net\", default= 'inception_v4')\n",
    "(options, args) = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# In each case, the benchmark of test process are same\n",
    "# ------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Faster Rcnn based on vgg16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = options.config_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zehaodong/Desktop/faster-rcnn-keras/Built_model\n"
     ]
    }
   ],
   "source": [
    "cd Built_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(file_name, 'rb') as f_name:\n",
    "    C = pickle.load(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vgg'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### This initilazer are according to data_generator.py #####\n",
    "'''\n",
    "Reference: https://github.com/yhenon/keras-frcnn\n",
    "'''\n",
    "# turn off any data augmentation at test time\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import raw feature extraction network from trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if C.network == 'vgg':\n",
    "    from keras_frcnn import vgg as nn\n",
    "elif C.network == 'resnet50':\n",
    "    from keras_frcnn import resnet as nn\n",
    "else:\n",
    "    from keras_frcnn import inception_V4 as nn   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other features int he trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4: 'cow', 15: 'diningtable', 20: 'bg', 18: 'aeroplane', 2: 'chair', 6: 'car', 7: 'motorbike', 16: 'train', 17: 'sofa', 13: 'boat', 10: 'cat', 3: 'tvmonitor', 14: 'bottle', 8: 'dog', 0: 'bus', 19: 'sheep', 5: 'bird', 9: 'horse', 1: 'person', 12: 'pottedplant', 11: 'bicycle'}\n"
     ]
    }
   ],
   "source": [
    "# map of class\n",
    "class_mapping = C.class_mapping\n",
    "if 'bg' not in class_mapping:\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\n",
    "C.num_rois = int(options.num_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if C.network == 'resnet50':\n",
    "    num_features = 1024\n",
    "elif C.network == 'vgg':\n",
    "    num_features = 512\n",
    "elif C.network == 'inception_v4':\n",
    "    num_features = 512\n",
    "else:\n",
    "    print('model error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numer of anchors\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trans_to_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "    # transform coordinate in the feature map back to the origin\n",
    "    real_x1 = int(round(x1 // ratio))\n",
    "    real_y1 = int(round(y1 // ratio))\n",
    "    real_x2 = int(round(x2 // ratio))\n",
    "    real_y2 = int(round(y2 // ratio))\n",
    "    return (real_x1, real_y1, real_x2 ,real_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_img(img, C):\n",
    "    #formats an image for model prediction based on config\n",
    "    img, ratio = format_img_size(img, C)\n",
    "    img = format_img_channels(img, C)\n",
    "    return img, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_img_size(img, C):\n",
    "    # formats the image size based on config\n",
    "    \"\"\"\n",
    "    Referencr: https://github.com/riadhayachi/faster-rcnn-keras\n",
    "    \"\"\"\n",
    "    img_min_side = float(C.im_size)\n",
    "    (height,width,_) = img.shape\n",
    "\n",
    "    if width <= height:\n",
    "        ratio = img_min_side/width\n",
    "        new_height = int(ratio * height)\n",
    "        new_width = int(img_min_side)\n",
    "    else:\n",
    "        ratio = img_min_side/height\n",
    "        new_width = int(ratio * width)\n",
    "        new_height = int(img_min_side)\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    return img, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_img_channels(img, C):\n",
    "    # formats the image channels based on config \n",
    "    \"\"\"\n",
    "    Referencr: https://github.com/riadhayachi/faster-rcnn-keras\n",
    "    \"\"\"\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = img.astype(np.float32)\n",
    "    img[:, :, 0] -= C.img_channel_mean[0]\n",
    "    img[:, :, 1] -= C.img_channel_mean[1]\n",
    "    img[:, :, 2] -= C.img_channel_mean[2]\n",
    "    img /= C.img_scaling_factor\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct faster RCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path = options.test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### this shape difference based on theano or tensorflow #####\n",
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape_img = (3, None, None)\n",
    "    input_shape_features = (num_features, None, None)\n",
    "else:\n",
    "    input_shape_img = (None, None, 3)\n",
    "    input_shape_features = (None, None, num_features)\n",
    "##### same input as we used to build the model in Frcnn_training\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frcnn network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### layers in FRCNN #########\n",
    "# raw feature extraction net, or shared CNN layer\n",
    "shared_CNN = nn.nn_base(img_input, trainable=True)\n",
    "# RPN layer\n",
    "rpn_layers = nn.rpn(shared_CNN, num_anchors)\n",
    "# classifier layer, from fast RCNN\n",
    "classifier_layer = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### model built by layer #########\n",
    "# RPN model\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "# classifier model \n",
    "model_classifier = Model([feature_map_input, roi_input], classifier_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# B: here uploads weights for each model\n",
    "#    choose a h5py file name: \n",
    "#          use it to get the weight of each model from traing process\n",
    "#         \n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "C.model_path = '/Users/zehaodong/Desktop/faster-rcnn-keras/Built_model/model_vgg16_weights.h5'\n",
    "#C.model_path = '/Users/zehaodong/Desktop/faster-rcnn-keras/Built_model/model_inception_weights.h5'\n",
    "#C.model_path = '/Users/zehaodong/Desktop/faster-rcnn-keras/Built_model/model_resnet_weights.h5'\n",
    "#C.model_path = '/Users/zehaodong/Desktop/faster-rcnn-keras/Built_model/model_best.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### import weights for model #########\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### add optimizer #########\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### initializer #########\n",
    "all_imgs = []\n",
    "classes = {}\n",
    "bbox_threshold = 0.8\n",
    "visualise = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### path set #####\n",
    "img_path = '/Users/zehaodong/Desktop/faster-rcnn-keras/VOCdevkit_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_path=os.path.join(img_path,'VOC2007')\n",
    "img_path=os.path.join(img_path,'JPEGImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dddd = os.listdir(img_path)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'005770.jpg'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_name = dddd[0]\n",
    "img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(img_path,img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/zehaodong/Desktop/faster-rcnn-keras/VOCdevkit_2/VOC2007/JPEGImages/005770.jpg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.rectangle(img, (98,100), (485,250), (0, 0, 255), 2)\n",
    "cv2.imwrite('/Users/zehaodong/Desktop/005770.png',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of iteration 0\n",
      "000008.jpg\n",
      "number of iteration 1\n",
      "000018.jpg\n",
      "number of iteration 2\n",
      "000022.jpg\n",
      "number of iteration 3\n",
      "000025.jpg\n",
      "number of iteration 4\n",
      "000027.jpg\n",
      "number of iteration 5\n",
      "000031.jpg\n",
      "number of iteration 6\n",
      "000037.jpg\n",
      "number of iteration 7\n",
      "000043.jpg\n",
      "number of iteration 8\n",
      "000045.jpg\n",
      "number of iteration 9\n",
      "000053.jpg\n",
      "number of iteration 10\n",
      "000057.jpg\n",
      "number of iteration 11\n",
      "000079.jpg\n",
      "number of iteration 12\n",
      "000080.jpg\n",
      "number of iteration 13\n",
      "000084.jpg\n",
      "number of iteration 14\n",
      "000085.jpg\n",
      "number of iteration 15\n",
      "000086.jpg\n",
      "number of iteration 16\n",
      "000087.jpg\n",
      "number of iteration 17\n",
      "000090.jpg\n",
      "number of iteration 18\n",
      "000092.jpg\n",
      "number of iteration 19\n",
      "000094.jpg\n",
      "number of iteration 20\n",
      "000119.jpg\n",
      "number of iteration 21\n",
      "000124.jpg\n",
      "number of iteration 22\n",
      "000126.jpg\n",
      "number of iteration 23\n",
      "000127.jpg\n",
      "number of iteration 24\n",
      "000137.jpg\n",
      "number of iteration 25\n",
      "000144.jpg\n",
      "number of iteration 26\n",
      "000145.jpg\n",
      "number of iteration 27\n",
      "000151.jpg\n",
      "number of iteration 28\n",
      "000152.jpg\n",
      "number of iteration 29\n",
      "000155.jpg\n",
      "number of iteration 30\n",
      "000157.jpg\n",
      "number of iteration 31\n",
      "000168.jpg\n",
      "number of iteration 32\n",
      "000178.jpg\n",
      "number of iteration 33\n",
      "000179.jpg\n",
      "number of iteration 34\n",
      "000181.jpg\n",
      "number of iteration 35\n",
      "000182.jpg\n",
      "number of iteration 36\n",
      "000183.jpg\n",
      "number of iteration 37\n",
      "000185.jpg\n",
      "number of iteration 38\n",
      "000186.jpg\n",
      "number of iteration 39\n",
      "000191.jpg\n",
      "number of iteration 40\n",
      "000195.jpg\n",
      "number of iteration 41\n",
      "000196.jpg\n",
      "number of iteration 42\n",
      "000197.jpg\n",
      "number of iteration 43\n",
      "000223.jpg\n",
      "number of iteration 44\n",
      "000226.jpg\n",
      "number of iteration 45\n",
      "000227.jpg\n",
      "number of iteration 46\n",
      "000230.jpg\n",
      "number of iteration 47\n",
      "000231.jpg\n",
      "number of iteration 48\n",
      "000234.jpg\n",
      "number of iteration 49\n",
      "000237.jpg\n",
      "number of iteration 50\n",
      "000247.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-539bdc81c357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# get the feature maps and output from the RPN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroi_helpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn_to_roi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_dim_ordering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# convert from (x1,y1,x2,y2) to (x,y,w,h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1790\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### test process through visualization #####\n",
    "\"\"\"\n",
    "Reference: py-faster-rcnn (python + caffe, https://github.com/rbgirshick/py-faster-rcnn)\n",
    "choose first 100 to see their performane\n",
    "for conveniance to generate html file, just use 20\n",
    "\"\"\"\n",
    "\n",
    "for idx, img_name in enumerate(sorted(os.listdir(img_path)[:2000])):\n",
    "    print('number of iteration', idx)\n",
    "    if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "        continue\n",
    "    print(img_name)\n",
    "    st = time.time()\n",
    "    filepath = os.path.join(img_path,img_name)\n",
    "    img = cv2.imread(filepath)\n",
    "    X, ratio = format_img(img, C)\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        X = np.transpose(X, (0, 2, 3, 1))\n",
    "    # get the feature maps and output from the RPN\n",
    "    [Y1, Y2, F] = model_rpn.predict(X)\n",
    "    R = roi_helpers.rpn_to_roi(Y1, Y2, C, K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "    # convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "    R[:, 2] -= R[:, 0]\n",
    "    R[:, 3] -= R[:, 1]\n",
    "    # apply the spatial pyramid pooling to the proposed regions\n",
    "    bboxes = {}\n",
    "    probs = {}\n",
    "    for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "        ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "        if ROIs.shape[1] == 0:\n",
    "            break\n",
    "        if jk == R.shape[0]//C.num_rois:\n",
    "            #pad R\n",
    "            curr_shape = ROIs.shape\n",
    "            target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "            ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "            ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "            ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "            ROIs = ROIs_padded\n",
    "        [P_cls, P_regr] = model_classifier.predict([F, ROIs])\n",
    "        for ii in range(P_cls.shape[1]):\n",
    "            if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                continue\n",
    "            cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "            if cls_name not in bboxes:\n",
    "                bboxes[cls_name] = []\n",
    "                probs[cls_name] = []\n",
    "            (x, y, w, h) = ROIs[0, ii, :]\n",
    "            cls_num = np.argmax(P_cls[0, ii, :])\n",
    "            try:\n",
    "                (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                tx /= C.classifier_regr_std[0]\n",
    "                ty /= C.classifier_regr_std[1]\n",
    "                tw /= C.classifier_regr_std[2]\n",
    "                th /= C.classifier_regr_std[3]\n",
    "                x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "            except:\n",
    "                pass\n",
    "            bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "            probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "    all_dets = []\n",
    "    for key in bboxes:\n",
    "        bbox = np.array(bboxes[key])\n",
    "        new_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n",
    "        for jk in range(new_boxes.shape[0]):\n",
    "            (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "            (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "            cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n",
    "            textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "            all_dets.append((key,100*new_probs[jk]))\n",
    "            (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "            textOrg = (real_x1, real_y1-0)\n",
    "            cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n",
    "            cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "            cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "    cv2.imwrite('/Users/zehaodong/Desktop/faster-rcnn-keras/results_vgg_imgs/{}.png'.format(idx),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "32\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n",
      "wow\n"
     ]
    }
   ],
   "source": [
    "for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "    if ROIs.shape[1] == 0:\n",
    "        break\n",
    "    if jk == R.shape[0]//C.num_rois:\n",
    "        #pad R\n",
    "        curr_shape = ROIs.shape\n",
    "        target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "        ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "        ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "        ROIs = ROIs_padded\n",
    "    [P_cls, P_regr] = model_classifier.predict([F, ROIs])\n",
    "    print(P_cls.shape[1])\n",
    "    for ii in range(P_cls.shape[1]):\n",
    "        if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "            print('wow')\n",
    "            continue\n",
    "        cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "        print(cls_name)\n",
    "        if cls_name not in bboxes:\n",
    "            bboxes[cls_name] = []\n",
    "            probs[cls_name] = []\n",
    "        (x, y, w, h) = ROIs[0, ii, :]\n",
    "        cls_num = np.argmax(P_cls[0, ii, :])\n",
    "\n",
    "        (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "        tx /= C.classifier_regr_std[0]\n",
    "        ty /= C.classifier_regr_std[1]\n",
    "        tw /= C.classifier_regr_std[2]\n",
    "        th /= C.classifier_regr_std[3]\n",
    "        x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "\n",
    "        bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "        probs[cls_name].append(np.max(P_cls[0, ii, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shape[0]//C.num_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.num_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'bus',\n",
       " 1: 'person',\n",
       " 2: 'chair',\n",
       " 3: 'tvmonitor',\n",
       " 4: 'cow',\n",
       " 5: 'bird',\n",
       " 6: 'car',\n",
       " 7: 'motorbike',\n",
       " 8: 'dog',\n",
       " 9: 'horse',\n",
       " 10: 'cat',\n",
       " 11: 'bicycle',\n",
       " 12: 'pottedplant',\n",
       " 13: 'boat',\n",
       " 14: 'bottle',\n",
       " 15: 'diningtable',\n",
       " 16: 'train',\n",
       " 17: 'sofa',\n",
       " 18: 'aeroplane',\n",
       " 19: 'sheep',\n",
       " 20: 'bg'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dets = []\n",
    "for key in bboxes:\n",
    "    bbox = np.array(bboxes[key])\n",
    "    new_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n",
    "    for jk in range(new_boxes.shape[0]):\n",
    "        (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "        (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "        cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n",
    "        textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "        all_dets.append((key,100*new_probs[jk]))\n",
    "        (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "        textOrg = (real_x1, real_y1-0)\n",
    "        cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n",
    "        cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "        cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "cv2.imwrite('/Users/zehaodong/Desktop/faster-rcnn-keras/results_vgg_imgs/{}.png'.format(idx),img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
